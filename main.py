import asyncio
import json
import re
import os

from typing import TypedDict, Annotated, Optional, Literal, List
from dotenv import load_dotenv

from langchain.agents import AgentExecutor, create_structured_chat_agent
from langchain.chat_models import init_chat_model
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableWithMessageHistory
from langchain_core.tools import BaseTool
from langchain_mcp_adapters.client import MultiServerMCPClient

from langgraph.graph import add_messages, StateGraph, END
from langgraph.prebuilt import create_react_agent

from db.chats import get_session_history
from template import system_template, router_template

load_dotenv()
api_key = os.getenv("BAILIAN_API_KEY")

# åˆå§‹åŒ–æ¨¡å‹
model = init_chat_model(
    "qwen3-0.6b",
    model_provider="openai",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=api_key,
    temperature=1,
    model_kwargs={"extra_body": {"enable_thinking": False}}
)

prompt = ChatPromptTemplate.from_messages([
    ("system", system_template),
    MessagesPlaceholder("chat_history"),
    ("human", "{input} {agent_scratchpad}"),
])


class AgentState(TypedDict):
    """çŠ¶æ€å®šä¹‰"""
    messages: Annotated[list, add_messages]
    sub_tasks: List[dict]  # [{"task": "xxx", "category": "xxx"}]
    current_idx: int  # å½“å‰æ‰§è¡Œåˆ°ç¬¬å‡ ä¸ªä»»åŠ¡
    is_chat: bool  # æ˜¯å¦ä¸ºæ™®é€šå¯¹è¯
    context_info: str  # ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æŸ¥è¯¢ç»“æœï¼‰
    input: str  # åŸå§‹ç”¨æˆ·è¾“å…¥

_tools_cache = None

async def load_mcp_tools():
    """åŠ è½½ MCP å·¥å…·"""
    global _tools_cache
    if _tools_cache is None:
        client = MultiServerMCPClient(
            {
                "wisehome": {
                    "url": "http://localhost:8001/sse",
                    "transport": "sse",
                }
            }
        )
        _tools_cache = await client.get_tools()
    return _tools_cache


def extract_json(text: str) -> dict:
    """ä»æ–‡æœ¬ä¸­æå– JSON"""
    text = text.strip()

    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    match = re.search(r'\{.*\}', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(0))
        except Exception as e:
            print(f"JSON è§£æå¤±è´¥ï¼š{e}")

    return {}


async def llm_route(user_input: str, context_info: str = "") -> dict:
    """ä½¿ç”¨ LLM åˆ¤æ–­æ„å›¾å¹¶æ‹†åˆ†ä»»åŠ¡"""
    full_input = f"{user_input}\n{context_info}" if context_info else user_input   # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé™„åŠ åˆ°è¾“å…¥ä¸­

    route_prompt = ChatPromptTemplate.from_messages([
        ("system", router_template),
        ("user", "{input}")
    ])

    response = await (route_prompt | model).ainvoke({"input": full_input})
    result = extract_json(response.content)

    if not result:
        print("è·¯ç”±è§£æå¤±è´¥ï¼Œé»˜è®¤ä¸ºæ™®é€šå¯¹è¯")
        return {"type": "chat", "response": "æŠ±æ­‰ï¼Œæˆ‘æ²¡ç†è§£ä½ çš„æ„æ€ã€‚"}

    result_type = result.get("type", "chat")

    if result_type == "task" and not result.get("sub_tasks"):
        print("ä»»åŠ¡æ‹†åˆ†ä¸ºç©ºï¼Œé™çº§ä¸ºå¯¹è¯")
        return {"type": "chat", "response": "è¯·å‘Šè¯‰æˆ‘ä½ éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Ÿ"}

    return result


async def agent_router(state: AgentState) -> AgentState:
    """è·¯ç”±èŠ‚ç‚¹ï¼šåˆ¤æ–­æ„å›¾å¹¶åˆå§‹åŒ–çŠ¶æ€"""
    user_input = state["input"]
    context_info = state.get("context_info", "")

    route_result = await llm_route(user_input, context_info)

    result_type = route_result.get("type", "chat")

    print(f"æ„å›¾è¯†åˆ«ï¼š{result_type}")

    if result_type == "chat":
        response = route_result.get("response", "ä½ å¥½ï¼")
        print(f"å¯¹è¯å›å¤ï¼š{response}")
        return {
            **state,
            "is_chat": True,
            "sub_tasks": [],
            "current_idx": 0,
            "messages": [AIMessage(content=response)],
        }
    else:
        sub_tasks = route_result.get("sub_tasks", [])
        print(f"ä»»åŠ¡æ‹†åˆ†ï¼š{json.dumps(sub_tasks, ensure_ascii=False)}")
        return {
            **state,
            "is_chat": False,
            "sub_tasks": sub_tasks,
            "current_idx": 0,
            "messages": state.get("messages", [])  # ä¿ç•™å·²æœ‰æ¶ˆæ¯
        }


def route_decision(state: AgentState) -> Literal["smart_home_control", "query_info", "mixed", "re_route", "end"]:
    """å†³ç­–å‡½æ•°ï¼šæ ¹æ®å½“å‰ä»»åŠ¡çš„ category è·¯ç”±"""
    if state.get("is_chat", False):
        return "end"

    current_idx = state.get("current_idx", 0)
    sub_tasks = state.get("sub_tasks", [])

    if current_idx >= len(sub_tasks):
        return "end"

    current_task = sub_tasks[current_idx]
    category = current_task.get("category", "query_info")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è·¯ç”±
    if current_task.get("needs_re_route", False):
        print(f"éœ€è¦é‡æ–°è·¯ç”± (ä»»åŠ¡ {current_idx + 1}/{len(sub_tasks)})")
        return "re_route"

    print(f"è·¯ç”±åˆ°: {category} (ä»»åŠ¡ {current_idx + 1}/{len(sub_tasks)})")

    return category


async def filter_tools_by_category(all_tools: List[BaseTool], category: str) -> List[BaseTool]:
    """æ ¹æ®ç±»åˆ«ç­›é€‰å·¥å…·"""
    if category == "smart_home_control":
        keywords = ["turn", "set", "open", "close", "switch", "è°ƒèŠ‚", "å¼€", "å…³", "è®¾ç½®", "æ’­æ”¾", "play", "stop", "éŸ³ä¹"]
    elif category == "query_info":
        keywords = ["get", "query", "status", "weather", "list", "æŸ¥è¯¢", "è·å–", "çŠ¶æ€", "ä¿¡æ¯", "rooms", "room"]
    else:
        return all_tools

    filtered_tools = [
        tool for tool in all_tools
        if any(kw in tool.name.lower() or kw in tool.description.lower() for kw in keywords)
    ]

    return filtered_tools if filtered_tools else all_tools


# å…¨å±€ç¼“å­˜ï¼šcategory -> (agent, agent_executor)
_agent_cache = {}
_agent_cache_lock = asyncio.Lock()

async def get_agent_executor_for_category(category: str, model, tools, prompt):
    """è·å–æŒ‡å®šç±»åˆ«çš„ agent executorï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    global _agent_cache

    if category in _agent_cache:
        return _agent_cache[category]

    async with _agent_cache_lock:
        # åŒé‡æ£€æŸ¥ï¼Œé˜²æ­¢ç«æ€
        if category in _agent_cache:
            return _agent_cache[category]

        # åˆ›å»º agent å’Œ executor
        agent = create_structured_chat_agent(model, tools=tools, prompt=prompt)
        agent_executor = AgentExecutor.from_agent_and_tools(
            agent=agent,
            tools=tools,
            verbose=True,
            handle_parsing_errors=True,
        )

        # åŒ…è£…æˆå¸¦è®°å¿†çš„ Runnable
        agent_with_memory = RunnableWithMessageHistory(
            agent_executor,
            get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
        )

        # ç¼“å­˜
        _agent_cache[category] = agent_with_memory
        return agent_with_memory


async def execute_simple_task(state: AgentState, category: str) -> AgentState:
    """æ‰§è¡Œç®€å•ä»»åŠ¡ï¼ˆå•ä¸€æŸ¥è¯¢æˆ–æ§åˆ¶ï¼‰"""
    current_idx = state["current_idx"]
    sub_tasks = state["sub_tasks"]
    current_task = sub_tasks[current_idx]
    task_content = current_task["task"]

    print(f"æ‰§è¡Œä»»åŠ¡ {current_idx + 1}: {task_content} [{category}]")

    all_tools = await load_mcp_tools()
    filtered_tools = await filter_tools_by_category(all_tools, category)

    tool_names = [t.name for t in filtered_tools]
    print(f"ä½¿ç”¨å·¥å…·: {', '.join(tool_names)}")

    agent_with_memory = await get_agent_executor_for_category(category, model, filtered_tools, prompt)

    try:
        response = await agent_with_memory.ainvoke(
            {"input": task_content},
            config={"configurable": {"thread_id": "2", "session_id": "user_1"}}
        )
        result = response.get("output", "æ‰§è¡Œå®Œæˆ")

        print(f"ä»»åŠ¡å®Œæˆ: {result}")

        return {
            **state,
            "messages": [AIMessage(content=result)],
            "current_idx": current_idx + 1
        }
    except Exception as e:
        print(f"ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
        return {
            **state,
            "messages": [AIMessage(content=f"å¤±è´¥: {str(e)}")],
            "current_idx": current_idx + 1
        }


def extract_query_from_mixed_task(task: str) -> str:
    """ä»æ··åˆä»»åŠ¡ä¸­æå–æŸ¥è¯¢éƒ¨åˆ†"""
    if "æ‰€æœ‰æˆ¿é—´" in task or "å…¨éƒ¨æˆ¿é—´" in task:
        return "è·å–æ‰€æœ‰æˆ¿é—´åˆ—è¡¨"
    elif "æ‰€æœ‰è®¾å¤‡" in task or "å…¨éƒ¨è®¾å¤‡" in task:
        return "è·å–æ‰€æœ‰è®¾å¤‡åˆ—è¡¨"
    else:
        return task

async def mixed_task_agent(state: AgentState) -> AgentState:
    """æ··åˆä»»åŠ¡æ‰§è¡Œå™¨ï¼šå…ˆæŸ¥è¯¢ï¼Œå°†ç»“æœé™„åŠ åˆ°åŸå§‹ä»»åŠ¡ï¼Œç„¶åæ ‡è®°éœ€è¦é‡æ–°è·¯ç”±"""
    current_idx = state["current_idx"]
    sub_tasks = state["sub_tasks"]
    current_task = sub_tasks[current_idx]
    task_content = current_task["task"]

    print(f"æ‰§è¡Œæ··åˆä»»åŠ¡ {current_idx + 1}: {task_content}")
    print(f"ç­–ç•¥ï¼šå…ˆæŸ¥è¯¢ä¿¡æ¯ï¼Œå†åŸºäºç»“æœé‡æ–°æ‹†åˆ†ä»»åŠ¡")

    # ç¬¬ä¸€æ­¥ï¼šæ‰§è¡ŒæŸ¥è¯¢ï¼ˆè·å–æˆ¿é—´åˆ—è¡¨ç­‰ï¼‰
    all_tools = await load_mcp_tools()
    query_tools = await filter_tools_by_category(all_tools, "query_info")

    tool_names = [t.name for t in query_tools]
    print(f"ç”¨æŸ¥è¯¢å·¥å…·: {', '.join(tool_names)}")

    agent_with_memory = await get_agent_executor_for_category("query_info", model, query_tools, prompt)

    try:
        # æ„é€ æŸ¥è¯¢æŒ‡ä»¤ï¼ˆæå–æŸ¥è¯¢éƒ¨åˆ†ï¼‰
        query_instruction = extract_query_from_mixed_task(task_content)
        print(f"æŸ¥è¯¢æŒ‡ä»¤: {query_instruction}")

        response = await agent_with_memory.ainvoke(
            {"input": query_instruction},
            config={"configurable": {"thread_id": "2", "session_id": "user_1"}}
        )
        query_result = response.get("output", "")

        print(f"æŸ¥è¯¢å®Œæˆ: {query_result}")

        # å°†æŸ¥è¯¢ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ï¼Œæ ‡è®°éœ€è¦é‡æ–°è·¯ç”±
        updated_task = {
            **current_task,
            "needs_re_route": True,
            "query_result": query_result
        }

        # æ›´æ–°ä»»åŠ¡åˆ—è¡¨
        updated_sub_tasks = sub_tasks.copy()
        updated_sub_tasks[current_idx] = updated_task

        # æ„é€ å®Œæ•´ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = f"è¡¥å……ä¿¡æ¯ï¼š{query_result}"

        return {
            **state,
            "sub_tasks": updated_sub_tasks,
            "context_info": context_info,
            "messages": [AIMessage(content=f"[æŸ¥è¯¢ç»“æœ] {query_result}")]
        }

    except Exception as e:
        print(f"æŸ¥è¯¢æ‰§è¡Œå‡ºé”™: {e}")
        return {
            **state,
            "messages": [AIMessage(content=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")],
            "current_idx": current_idx + 1
        }


async def re_route_agent(state: AgentState) -> AgentState:
    """é‡æ–°è·¯ç”±èŠ‚ç‚¹ï¼šåŸºäºæŸ¥è¯¢ç»“æœï¼Œé‡æ–°æ‹†åˆ†ä»»åŠ¡"""
    current_idx = state["current_idx"]
    sub_tasks = state["sub_tasks"]
    current_task = sub_tasks[current_idx]

    original_task = current_task["task"]
    query_result = current_task.get("query_result", "")

    print(f"é‡æ–°è·¯ç”±ä»»åŠ¡: {original_task}")
    print(f"åŸºäºæŸ¥è¯¢ç»“æœ: {query_result}")

    # ä½¿ç”¨ LLM é‡æ–°æ‹†åˆ†ä»»åŠ¡ï¼ˆé™„åŠ æŸ¥è¯¢ç»“æœä½œä¸ºä¸Šä¸‹æ–‡ï¼‰
    context_info = f"è¡¥å……ä¿¡æ¯ï¼š{query_result}"
    route_result = await llm_route(original_task, context_info)

    if route_result.get("type") == "task":
        new_sub_tasks = route_result.get("sub_tasks", [])
        print(f"æ‹†åˆ†ç»“æœï¼š{json.dumps(new_sub_tasks, ensure_ascii=False)}")

        # æ›¿æ¢å½“å‰ä»»åŠ¡ä¸ºæ–°æ‹†åˆ†çš„ä»»åŠ¡åˆ—è¡¨
        updated_sub_tasks = sub_tasks[:current_idx] + new_sub_tasks + sub_tasks[current_idx + 1:]

        return {
            **state,
            "sub_tasks": updated_sub_tasks,
            "context_info": "",
        }
    else:
        # æ‹†åˆ†å¤±è´¥ï¼Œè·³è¿‡æ­¤ä»»åŠ¡
        print("æ‹†åˆ†å¤±è´¥ï¼Œè·³è¿‡æ­¤ä»»åŠ¡")
        return {
            **state,
            "current_idx": current_idx + 1,
            "context_info": ""
        }


async def smart_home_agent(state: AgentState) -> AgentState:
    """æ™ºèƒ½å®¶å±…æ§åˆ¶æ‰§è¡Œå™¨"""
    return await execute_simple_task(state, "smart_home_control")


async def query_info_agent(state: AgentState) -> AgentState:
    """ä¿¡æ¯æŸ¥è¯¢æ‰§è¡Œå™¨"""
    return await execute_simple_task(state, "query_info")

async def create_workflow():
    """æ„å»º Workflow"""
    workflow = StateGraph(AgentState)

    workflow.add_node("router", agent_router)
    workflow.add_node("smart_home_control", smart_home_agent)
    workflow.add_node("query_info", query_info_agent)
    workflow.add_node("mixed", mixed_task_agent)
    workflow.add_node("re_route", re_route_agent)

    workflow.set_entry_point("router")

    workflow.add_conditional_edges(
        "router",
        route_decision,
        {
            "smart_home_control": "smart_home_control",
            "query_info": "query_info",
            "mixed": "mixed",
            "re_route": "re_route",
            "end": END
        }
    )

    workflow.add_conditional_edges(
        "smart_home_control",
        route_decision,
        {
            "smart_home_control": "smart_home_control",
            "query_info": "query_info",
            "mixed": "mixed",
            "re_route": "re_route",
            "end": END
        }
    )

    workflow.add_conditional_edges(
        "query_info",
        route_decision,
        {
            "smart_home_control": "smart_home_control",
            "query_info": "query_info",
            "mixed": "mixed",
            "re_route": "re_route",
            "end": END
        }
    )

    workflow.add_conditional_edges(
        "mixed",
        route_decision,
        {
            "smart_home_control": "smart_home_control",
            "query_info": "query_info",
            "mixed": "mixed",
            "re_route": "re_route",
            "end": END
        }
    )

    workflow.add_conditional_edges(
        "re_route",
        route_decision,
        {
            "smart_home_control": "smart_home_control",
            "query_info": "query_info",
            "mixed": "mixed",
            "re_route": "re_route",
            "end": END
        }
    )

    return workflow.compile()


async def main():
    try:
        print("MCP æ™ºèƒ½å®¶å±…ç³»ç»Ÿå·²è¿æ¥ï¼")
        print("ç¤ºä¾‹ï¼š'æ‰“å¼€å®¢å…ç¯'ï¼Œ'æŸ¥è¯¢å¤©æ°”'")

        workflow = await create_workflow()

        while True:
            user_input = input("\nä½ : ").strip()

            if user_input.lower() in {"exit", "quit", "é€€å‡º"}:
                print("ğŸ‘‹ å†è§ï¼")
                break

            if not user_input:
                continue

            response = await workflow.ainvoke(
                {"input": user_input, "context_info": ""},
                config={"configurable": {"thread_id": "2", "session_id": "user_1"}}
            )

            messages = response.get("messages", [])
            if messages:
                print("\nAI:")
                for msg in messages:
                    if hasattr(msg, "content"):
                        print(f"{msg.content}")
            else:
                print("AI: (æ— å›å¤)")

    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯ï¼š{e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nå·²é€€å‡ºã€‚")