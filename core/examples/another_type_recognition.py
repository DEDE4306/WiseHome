import asyncio
import queue
import threading
import numpy as np
import sounddevice as sd
from funasr import AutoModel
from pathlib import Path
from typing import List, Tuple, Optional

# æ¨¡å‹ç¼“å­˜ç›®å½•ï¼›å¦‚æœèƒ½å¤Ÿè·å–åˆ°ç¼“å­˜åˆ™ä½¿ç”¨ç¼“å­˜ç›®å½•ä¸­çš„æ¨¡å‹ï¼Œå¦åˆ™ä»äº’è”ç½‘ä¸‹è½½æ¨¡å‹
MODEL_CACHE_DIR = "model"

VAD_MODEL = "fsmn-vad"
ASR_MODEL = "SenseVoiceSmall"

VAD_DIR = Path(MODEL_CACHE_DIR) / VAD_MODEL
ASR_DIR = Path(MODEL_CACHE_DIR) / ASR_MODEL

# éŸ³é¢‘é…ç½®
SAMPLE_RATE = 16000
CHANNELS = 1
BLOCK_SIZE = 9600  # çº¦ 0.6 ç§’ (9600 / 16000)

# VAD é…ç½®
VAD_CHUNK_SIZE = 9600  # VAD æ¨¡å‹å—å¤§å°
MAX_RECORDING_DURATION = 30.0  # æœ€å¤§å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰
SILENCE_AFTER_SPEECH = 1.0  # è¯­éŸ³ç»“æŸåçš„é™éŸ³ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰


class VoiceRecognizer:
    def __init__(self):
        print("æ­£åœ¨åŠ è½½ VAD æ¨¡å‹...")
        self.vad_model = AutoModel(model=VAD_DIR, disable_pbar=True)
        print("æ­£åœ¨åŠ è½½ ASR æ¨¡å‹...")
        self.asr_model = AutoModel(model=ASR_DIR, disable_pbar=True)
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        self.cache = {}
        self.audio_buffer = []
        self.vad_segments = []
        self.is_listening = False
        self.recording_start_time = None
        
        # å¼‚æ­¥å¤„ç†é˜Ÿåˆ—
        self.audio_queue = queue.Queue()
        self.processing_thread = None
        self.processing_stop_event = threading.Event()

    async def get_voice_input(self) -> str:
        """å¯åŠ¨ä¸€æ¬¡è¯­éŸ³è¾“å…¥ï¼Œè¿”å›è¯†åˆ«å‡ºçš„å®Œæ•´å¥å­"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._record_and_recognize)

    def _record_and_recognize(self) -> str:
        """å½•éŸ³å¹¶è¯†åˆ«è¯­éŸ³"""
        self.is_listening = True
        self.audio_buffer = []
        self.vad_segments = []
        self.recording_start_time = None
        self.cache = {}
        
        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        self.processing_stop_event.clear()
        self.processing_thread = threading.Thread(target=self._processing_worker)
        self.processing_thread.start()

        # å¯åŠ¨å½•éŸ³æµ
        with sd.InputStream(
            samplerate=SAMPLE_RATE,
            channels=CHANNELS,
            dtype=np.float32,
            callback=self._audio_callback,
            blocksize=BLOCK_SIZE
        ):
            print("ğŸ¤ è¯·è¯´è¯...")
            # ç­‰å¾…è¯­éŸ³ç»“æŸ
            while self.is_listening:
                sd.sleep(100)

        # åœæ­¢å¤„ç†çº¿ç¨‹
        self.processing_stop_event.set()
        if self.processing_thread:
            self.processing_thread.join(timeout=2)

        # æå–è¯­éŸ³ç‰‡æ®µå¹¶è¯†åˆ«
        return self._extract_and_recognize()

    def _audio_callback(self, indata, frames, time, status):
        """éŸ³é¢‘æµå›è°ƒ"""
        if status:
            if status.input_overflow:
                if not hasattr(self, '_overflow_logged'):
                    print("è­¦å‘Š: éŸ³é¢‘ç¼“å†²åŒºæº¢å‡ºï¼Œè€ƒè™‘å¢å¤§ blocksize æˆ–å‡å°‘å¤„ç†é‡")
                    self._overflow_logged = True
            else:
                print(f"Audio status: {status}")

        audio_chunk = indata[:, 0]
        
        # å°†éŸ³é¢‘å—æ”¾å…¥é˜Ÿåˆ—ï¼Œå¼‚æ­¥å¤„ç†
        self.audio_queue.put({
            'audio_chunk': audio_chunk,
            'timestamp': time.currentTime
        })

    def _processing_worker(self):
        """åå°å¤„ç†çº¿ç¨‹ - å¤„ç† VAD æ£€æµ‹é€»è¾‘"""
        while not self.processing_stop_event.is_set():
            try:
                # ä»é˜Ÿåˆ—è·å–æ•°æ®ï¼Œè¶…æ—¶ 0.1 ç§’
                data = self.audio_queue.get(timeout=0.1)
                
                audio_chunk = data['audio_chunk']
                current_time = data['timestamp']
                
                # ä¿å­˜éŸ³é¢‘åˆ°ç¼“å†²åŒº
                self.audio_buffer.append(audio_chunk)
                
                # è®°å½•å½•éŸ³å¼€å§‹æ—¶é—´
                if self.recording_start_time is None:
                    self.recording_start_time = current_time
                
                # ä½¿ç”¨ VAD æ¨¡å‹æ£€æµ‹è¯­éŸ³ç«¯ç‚¹
                self._detect_vad_segments(audio_chunk, current_time)
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»“æŸå½•éŸ³
                self._check_recording_end(current_time)
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"å¤„ç†çº¿ç¨‹é”™è¯¯: {e}")

    def _detect_vad_segments(self, audio_chunk: np.ndarray, current_time: float):
        """ä½¿ç”¨ VAD æ¨¡å‹æ£€æµ‹è¯­éŸ³ç«¯ç‚¹"""
        try:
            res = self.vad_model.generate(
                input=audio_chunk,
                cache=self.cache,
                is_final=False,
                chunk_size=VAD_CHUNK_SIZE,
            )
            
            if res and len(res) > 0:
                segments = res[0].get("value", [])
                
                # VAD è¾“å‡ºæ ¼å¼ï¼š
                # [[beg1, end1], [beg2, end2], .., [begN, endN]]ï¼šæ£€æµ‹åˆ°è¯­éŸ³
                # [[beg, -1]]ï¼šåªæ£€æµ‹åˆ°èµ·å§‹ç‚¹
                # [[-1, end]]ï¼šåªæ£€æµ‹åˆ°ç»“æŸç‚¹
                # []ï¼šæ²¡æœ‰æ£€æµ‹åˆ°èµ·å§‹ç‚¹å’Œç»“æŸç‚¹
                # è¾“å‡ºç»“æœå•ä½ä¸ºæ¯«ç§’ï¼Œä»èµ·å§‹ç‚¹å¼€å§‹çš„ç»å¯¹æ—¶é—´
                
                if segments and len(segments) > 0:
                    for segment in segments:
                        beg_ms, end_ms = segment
                        
                        # è½¬æ¢ä¸ºç§’
                        beg_sec = beg_ms / 1000.0 if beg_ms >= 0 else -1
                        end_sec = end_ms / 1000.0 if end_ms >= 0 else -1
                        
                        # è®¡ç®—ç›¸å¯¹äºå½•éŸ³å¼€å§‹çš„æ—¶é—´
                        if self.recording_start_time is not None:
                            relative_time = current_time - self.recording_start_time
                            
                            # å¤„ç†èµ·å§‹ç‚¹
                            if beg_sec >= 0:
                                abs_beg = relative_time + beg_sec
                                self._add_vad_segment(abs_beg, None)
                            
                            # å¤„ç†ç»“æŸç‚¹
                            if end_sec >= 0:
                                abs_end = relative_time + end_sec
                                self._add_vad_segment(None, abs_end)
                        
        except Exception as e:
            print(f"VAD æ£€æµ‹é”™è¯¯: {e}")

    def _add_vad_segment(self, beg: Optional[float], end: Optional[float]):
        """æ·»åŠ  VAD ç«¯ç‚¹"""
        if beg is not None:
            # æŸ¥æ‰¾æ˜¯å¦æœ‰æœªç»“æŸçš„ç‰‡æ®µ
            for i, (s_beg, s_end) in enumerate(self.vad_segments):
                if s_end is None:
                    # å·²æœ‰èµ·å§‹ç‚¹ï¼Œå¿½ç•¥æ–°çš„èµ·å§‹ç‚¹
                    return
            # æ·»åŠ æ–°çš„èµ·å§‹ç‚¹
            self.vad_segments.append([beg, None])
        
        if end is not None:
            # æŸ¥æ‰¾æœ€è¿‘çš„æœªç»“æŸç‰‡æ®µ
            for i in range(len(self.vad_segments) - 1, -1, -1):
                s_beg, s_end = self.vad_segments[i]
                if s_end is None:
                    self.vad_segments[i][1] = end
                    return
            # æ²¡æœ‰æ‰¾åˆ°èµ·å§‹ç‚¹ï¼Œå¿½ç•¥ç»“æŸç‚¹

    def _check_recording_end(self, current_time: float):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»“æŸå½•éŸ³"""
        if not self.vad_segments:
            # è¿˜æ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³ï¼Œç»§ç»­å½•éŸ³
            return
        
        # æ£€æŸ¥æœ€åä¸€ä¸ªè¯­éŸ³ç‰‡æ®µæ˜¯å¦ç»“æŸ
        last_segment = self.vad_segments[-1]
        if last_segment[1] is not None:
            # æœ€åä¸€ä¸ªç‰‡æ®µå·²ç»“æŸï¼Œæ£€æŸ¥æ˜¯å¦è¿‡äº†è¶³å¤Ÿçš„é™éŸ³æ—¶é—´
            silence_duration = current_time - last_segment[1]
            if silence_duration >= SILENCE_AFTER_SPEECH:
                # æ£€æŸ¥æ€»å½•éŸ³æ—¶é•¿
                if self.recording_start_time is not None:
                    total_duration = current_time - self.recording_start_time
                    if total_duration >= 1.0:  # è‡³å°‘å½•éŸ³ 1 ç§’
                        print(f"è¯­éŸ³ç»“æŸï¼Œæ€»æ—¶é•¿: {total_duration:.2f}ç§’")
                        self.is_listening = False

    def _extract_and_recognize(self) -> str:
        """æå–è¯­éŸ³ç‰‡æ®µå¹¶è¯†åˆ«"""
        if not self.audio_buffer:
            print("æ²¡æœ‰å½•åˆ¶åˆ°éŸ³é¢‘")
            return ""
        
        # åˆå¹¶æ‰€æœ‰éŸ³é¢‘å—
        full_audio = np.concatenate(self.audio_buffer)
        
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³ç‰‡æ®µï¼Œè¿”å›ç©º
        if not self.vad_segments:
            print("æœªæ£€æµ‹åˆ°è¯­éŸ³ç‰‡æ®µ")
            return ""
        
        # æå–æœ‰æ•ˆçš„è¯­éŸ³ç‰‡æ®µ
        speech_segments = []
        for beg, end in self.vad_segments:
            if end is not None and beg < end:
                # è½¬æ¢ä¸ºæ ·æœ¬ç´¢å¼•
                beg_sample = int(beg * SAMPLE_RATE)
                end_sample = int(end * SAMPLE_RATE)
                
                # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                beg_sample = max(0, beg_sample)
                end_sample = min(len(full_audio), end_sample)
                
                if beg_sample < end_sample:
                    segment = full_audio[beg_sample:end_sample]
                    speech_segments.append(segment)
        
        if not speech_segments:
            print("æ²¡æœ‰æœ‰æ•ˆçš„è¯­éŸ³ç‰‡æ®µ")
            return ""
        
        # åˆå¹¶æ‰€æœ‰è¯­éŸ³ç‰‡æ®µ
        speech_audio = np.concatenate(speech_segments)
        
        print(f"æå–åˆ° {len(speech_segments)} ä¸ªè¯­éŸ³ç‰‡æ®µï¼Œæ€»æ—¶é•¿: {len(speech_audio) / SAMPLE_RATE:.2f}ç§’")
        
        # ä½¿ç”¨éæµå¼ ASR è¯†åˆ«
        return self._recognize(speech_audio)

    def _recognize(self, audio: np.ndarray) -> str:
        """é€šè¿‡ ASR æ¨¡å‹è¯†åˆ«è¯­éŸ³ï¼ˆéæµå¼ï¼‰"""
        try:
            print("æ­£åœ¨è¯†åˆ«è¯­éŸ³...")
            result = self.asr_model.generate(
                input=audio,
                batch_size=1
            )
            
            if result and len(result) > 0:
                text = result[0].get("text", "").strip()
                print(f"âœ… è¯†åˆ«ç»“æœ: '{text}'")
                return text
            
            return ""
            
        except Exception as e:
            print(f"ASR æ¨¡å‹è¯†åˆ«é”™è¯¯: {e}")
            return ""


# å…¨å±€å®ä¾‹
_recognizer = None

async def get_voice_input() -> str:
    global _recognizer
    if _recognizer is None:
        _recognizer = VoiceRecognizer()
    return await _recognizer.get_voice_input()

async def main():
    while True:
        text = await get_voice_input()
        print(f"è¯†åˆ«ç»“æœ: {text}")

if __name__ == "__main__":
    asyncio.run(main())