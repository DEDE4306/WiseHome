# voice_input.py
import asyncio
import queue
import threading
import numpy as np
import sounddevice as sd
from funasr import AutoModel

# å…¨å±€é…ç½®
SAMPLE_RATE = 16000
CHANNELS = 1
BLOCK_SIZE = 9600  # çº¦ 0.6 ç§’ (9600 / 16000)

# å…¨å±€æ¨¡åž‹ï¼ˆæ‡’åŠ è½½ï¼‰
_recognizer = None


class SimpleVoiceRecognizer:
    def __init__(self):
        print("æ­£åœ¨åŠ è½½è¯­éŸ³è¯†åˆ«æ¨¡åž‹...")
        self.model = AutoModel(
            model="D:/Project/WiseHome/model/paraformer-zh-streaming",
            disable_pbar=True
        )
        self.cache = {}
        self.result_text = ""
        self.final_result = ""
        self.is_listening = False
        
        # è¯­éŸ³æ£€æµ‹å‚æ•°
        self.silence_threshold = 0.005  # é™éŸ³é˜ˆå€¼ï¼ˆå¯è°ƒæ•´ï¼‰
        self.silence_duration = 1.0     # é™éŸ³æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
        self.min_speech_duration = 0.3  # æœ€å°è¯­éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰
        self.debug_mode = False         # è°ƒè¯•æ¨¡å¼
        
        # çŠ¶æ€å˜é‡
        self.speech_detected = False    # æ˜¯å¦æ£€æµ‹åˆ°è¯­éŸ³
        self.speech_start_time = None   # è¯­éŸ³å¼€å§‹æ—¶é—´
        self.silence_start_time = None  # é™éŸ³å¼€å§‹æ—¶é—´
        self.audio_buffer = []          # éŸ³é¢‘ç¼“å†²åŒºï¼ˆç”¨äºŽæœ€ç»ˆè¯†åˆ«ï¼‰
        self.last_recognition_time = 0  # ä¸Šæ¬¡è¯†åˆ«æ—¶é—´
        
        # å¼‚æ­¥å¤„ç†é˜Ÿåˆ—
        self.processing_queue = queue.Queue()
        self.processing_thread = None
        self.processing_stop_event = threading.Event()


    # è¿™å¥è¯ä¸çŸ¥é“ä»€ä¹ˆæ„æ€ï¼Œä¸ºä»€ä¹ˆæœ‰äº‹ä»¶å¾ªçŽ¯è¿™ç§è®©äººæ‘¸ä¸æ¸…å¤´è„‘çš„ä¸œè¥¿
    async def get_voice_input(self) -> str:
        """å¯åŠ¨ä¸€æ¬¡è¯­éŸ³è¾“å…¥ï¼Œè¿”å›žè¯†åˆ«å‡ºçš„å®Œæ•´å¥å­"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._record_and_recognize)

    def _record_and_recognize(self) -> str:
        """å½•éŸ³å¹¶è¯†åˆ«è¯­éŸ³"""
        self.final_result = ""
        self.is_listening = True
        
        # é‡ç½®çŠ¶æ€
        self.speech_detected = False
        self.speech_start_time = None
        self.silence_start_time = None
        self.audio_buffer = []
        self.cache = {}
        
        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        self.processing_stop_event.clear()
        self.processing_thread = threading.Thread(target=self._processing_worker)
        self.processing_thread.start()

        # å¯åŠ¨å½•éŸ³æµ
        with sd.InputStream(
            samplerate=SAMPLE_RATE,
            channels=CHANNELS,
            dtype=np.float32,
            callback=self._audio_callback,
            blocksize=BLOCK_SIZE
        ):
            print("ðŸŽ¤ è¯·è¯´è¯ï¼ˆè¯´å®ŒåŽåœé¡¿1ç§’ï¼‰...")
            # ç­‰å¾…è¯­éŸ³ç»“æŸ
            while self.is_listening:
                sd.sleep(100)

        # åœæ­¢å¤„ç†çº¿ç¨‹
        self.processing_stop_event.set()
        if self.processing_thread:
            self.processing_thread.join(timeout=2)

        print(f"âœ… è¯†åˆ«å®Œæˆ: '{self.final_result}'")
        return self.final_result.strip()

    def _audio_callback(self, indata, frames, time, status):
        """éŸ³é¢‘å›žè°ƒå‡½æ•° - åªåšè½»é‡çº§å¤„ç†"""
        if status:
            if status.input_overflow:
                # åªåœ¨ç¬¬ä¸€æ¬¡æº¢å‡ºæ—¶æ‰“å°
                if not hasattr(self, '_overflow_logged'):
                    print("è­¦å‘Š: éŸ³é¢‘ç¼“å†²åŒºæº¢å‡ºï¼Œè€ƒè™‘å¢žå¤§ blocksize æˆ–å‡å°‘å¤„ç†é‡")
                    self._overflow_logged = True
            else:
                print(f"Audio status: {status}")

        audio_chunk = indata[:, 0]
        
        # è®¡ç®—éŸ³é‡ï¼ˆRMSï¼‰
        volume = np.sqrt(np.mean(audio_chunk ** 2))
        
        # å°†éŸ³é¢‘å—å’Œå¤„ç†ä¿¡æ¯æ”¾å…¥é˜Ÿåˆ—ï¼Œå¼‚æ­¥å¤„ç†
        self.processing_queue.put({
            'audio_chunk': audio_chunk,
            'volume': volume,
            'timestamp': time.currentTime
        })

    def _processing_worker(self):
        """åŽå°å¤„ç†çº¿ç¨‹ - å¤„ç†éŸ³é¢‘è¯†åˆ«é€»è¾‘"""
        while not self.processing_stop_event.is_set():
            try:
                # ä»Žé˜Ÿåˆ—èŽ·å–æ•°æ®ï¼Œè¶…æ—¶ 0.1 ç§’
                data = self.processing_queue.get(timeout=0.1)
                
                audio_chunk = data['audio_chunk']
                volume = data['volume']
                current_time = data['timestamp']
                
                # ä¿å­˜éŸ³é¢‘åˆ°ç¼“å†²åŒº
                self.audio_buffer.append(audio_chunk)
                
                # è°ƒè¯•æ—¥å¿—
                if self.debug_mode:
                    print(f"[DEBUG] éŸ³é‡: {volume:.6f}, è¯­éŸ³æ£€æµ‹: {self.speech_detected}")
                
                # æ£€æµ‹è¯­éŸ³çŠ¶æ€
                if volume > self.silence_threshold:
                    # æ£€æµ‹åˆ°è¯­éŸ³
                    if not self.speech_detected:
                        self.speech_detected = True
                        self.speech_start_time = current_time
                        self.silence_start_time = None
                        if self.debug_mode:
                            print("[DEBUG] æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹")
                    else:
                        # è¯­éŸ³ç»§ç»­ï¼Œé‡ç½®é™éŸ³è®¡æ—¶
                        self.silence_start_time = None
                else:
                    # æ£€æµ‹åˆ°é™éŸ³
                    if self.speech_detected and self.silence_start_time is None:
                        self.silence_start_time = current_time
                        if self.debug_mode:
                            print("[DEBUG] æ£€æµ‹åˆ°é™éŸ³å¼€å§‹")
                
                # å®žæ—¶è¯†åˆ«ï¼ˆå¯é€‰ï¼Œç”¨äºŽæ˜¾ç¤ºä¸­é—´ç»“æžœï¼‰
                if self.speech_detected:
                    try:
                        res = self.model.generate(
                            input=audio_chunk,
                            cache=self.cache,
                            is_final=False,
                            chunk_size=[0, 10, 5],
                            encoder_chunk_look_back=4,
                            decoder_chunk_look_back=1
                        )
                        
                        if res and len(res) > 0:
                            text = res[0].get("text", "").strip()
                            if text and text != self.result_text:
                                self.result_text = text
                                if self.debug_mode:
                                    print(f"[å®žæ—¶] {text}")
                    except Exception as e:
                        if self.debug_mode:
                            print(f"[DEBUG] å®žæ—¶è¯†åˆ«é”™è¯¯: {e}")
                
                # åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»“æŸ
                if self.speech_detected and self.silence_start_time is not None:
                    silence_duration = current_time - self.silence_start_time
                    speech_duration = current_time - self.speech_start_time
                    
                    # æ»¡è¶³ç»“æŸæ¡ä»¶ï¼šé™éŸ³æ—¶é—´è¶³å¤Ÿ ä¸” è¯­éŸ³æ—¶é—´è¶³å¤Ÿ
                    if (silence_duration >= self.silence_duration and 
                        speech_duration >= self.min_speech_duration):
                        if self.debug_mode:
                            print(f"[DEBUG] è¯­éŸ³ç»“æŸ - é™éŸ³æ—¶é•¿: {silence_duration:.2f}s, è¯­éŸ³æ—¶é•¿: {speech_duration:.2f}s")
                        self._finalize_recognition()
                        
            except queue.Empty:
                continue
            except Exception as e:
                print(f"å¤„ç†çº¿ç¨‹é”™è¯¯: {e}")
    
    def _finalize_recognition(self):
        """å®Œæˆè¯†åˆ«ï¼Œå¤„ç†ç¼“å†²åŒºä¸­çš„æ‰€æœ‰éŸ³é¢‘"""
        if not self.audio_buffer:
            self.is_listening = False
            return
        
        print("ðŸ”„ æ­£åœ¨å¤„ç†å®Œæ•´éŸ³é¢‘...")
        
        # åˆå¹¶æ‰€æœ‰éŸ³é¢‘å—
        full_audio = np.concatenate(self.audio_buffer)
        
        # ä½¿ç”¨å®Œæ•´çš„éŸ³é¢‘è¿›è¡Œæœ€ç»ˆè¯†åˆ«
        try:
            res = self.model.generate(
                input=full_audio,
                cache={},
                is_final=True,
                chunk_size=[0, 10, 5],
                encoder_chunk_look_back=4,
                decoder_chunk_look_back=1
            )
            
            if res and len(res) > 0:
                self.final_result = res[0].get("text", "").strip()
        except Exception as e:
            print(f"æœ€ç»ˆè¯†åˆ«é”™è¯¯: {e}")
            self.final_result = self.result_text
        
        # æ¸…ç†çŠ¶æ€
        self.audio_buffer = []
        self.speech_detected = False
        self.is_listening = False


# å…¨å±€å®žä¾‹
_recognizer = None

async def get_voice_input() -> str:
    global _recognizer
    if _recognizer is None:
        _recognizer = SimpleVoiceRecognizer()
    return await _recognizer.get_voice_input()

async def main():
    while True:
        text = await get_voice_input()
        print(text)

if __name__ == "__main__":
    asyncio.run(main())
