"""
è¯­éŸ³è¾“å…¥æ¨¡å— - å¢å¼ºç‰ˆ
æ­£ç¡®å¤„ç†VADæ¨¡å‹çš„è¾“å‡ºæ ¼å¼å’ŒçŠ¶æ€è½¬æ¢
"""
import sounddevice as sd
import numpy as np
from funasr import AutoModel
import asyncio
from typing import Optional, Tuple, List
import queue
import os
from datetime import datetime
import soundfile as sf

from voice_config import *


class EnhancedVoiceInputHandler:
    """å¢å¼ºçš„è¯­éŸ³è¾“å…¥å¤„ç†å™¨ - æ­£ç¡®å¤„ç†VADçŠ¶æ€"""
    
    def __init__(self):
        self.sr = SAMPLE_RATE
        self.vad_chunk_size = VAD_CHUNK_SIZE
        
        # åŠ è½½æ¨¡å‹
        if DEBUG:
            print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        self.vad_model = AutoModel(model=VAD_MODEL, disable_pbar=True)
        self.asr_model = AutoModel(model=ASR_MODEL)
        
        if DEBUG:
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # çŠ¶æ€
        self.audio_queue = queue.Queue()
        self.is_active = False
        
        # VADçŠ¶æ€è·Ÿè¸ª
        self.vad_speech_started = False  # VADæ˜¯å¦æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹
        self.vad_speech_ended = False    # VADæ˜¯å¦æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ
        
        # å¦‚æœéœ€è¦ä¿å­˜éŸ³é¢‘ï¼Œåˆ›å»ºç›®å½•
        if SAVE_AUDIO:
            os.makedirs(AUDIO_SAVE_PATH, exist_ok=True)
    
    def _audio_callback(self, indata, frames, time, status):
        """éŸ³é¢‘æµå›è°ƒ"""
        if status and DEBUG:
            print(f"âš ï¸ éŸ³é¢‘çŠ¶æ€: {status}")
        
        audio_chunk = indata[:, 0].copy()
        self.audio_queue.put(audio_chunk)
    
    def _check_volume(self, audio: np.ndarray) -> Tuple[float, float]:
        """æ£€æŸ¥éŸ³é‡"""
        volume = np.abs(audio).mean()
        volume_db = 20 * np.log10(volume + 1e-10)
        return volume, volume_db
    
    def _parse_vad_output(self, vad_segments: List) -> Tuple[bool, bool, bool]:
        """
        è§£æVADè¾“å‡º
        
        Args:
            vad_segments: VADæ¨¡å‹è¾“å‡ºçš„æ—¶é—´æ®µåˆ—è¡¨
            
        Returns:
            Tuple[bool, bool, bool]: (æœ‰è¯­éŸ³, æ£€æµ‹åˆ°å¼€å§‹, æ£€æµ‹åˆ°ç»“æŸ)
            
        VADè¾“å‡ºæ ¼å¼:
        - [[beg1, end1], [beg2, end2], ...]: å®Œæ•´çš„è¯­éŸ³æ®µ
        - [[beg, -1]]: åªæ£€æµ‹åˆ°èµ·å§‹ç‚¹
        - [[-1, end]]: åªæ£€æµ‹åˆ°ç»“æŸç‚¹
        - []: æ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³
        """
        if not vad_segments or len(vad_segments) == 0:
            # ç©ºåˆ—è¡¨ï¼šæ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³
            return False, False, False
        
        has_speech = False
        has_start = False
        has_end = False
        
        for segment in vad_segments:
            if len(segment) >= 2:
                beg, end = segment[0], segment[1]
                
                if beg >= 0 and end == -1:
                    # [[beg, -1]]: æ£€æµ‹åˆ°èµ·å§‹ç‚¹
                    has_speech = True
                    has_start = True
                
                elif beg >= 0 and end > beg:
                    # [[beg, end]]: å®Œæ•´çš„è¯­éŸ³æ®µ
                    has_speech = True
                    has_start = True
                    has_end = True
                
                elif beg == -1 and end >= 0:
                    # [[-1, end]]: æ£€æµ‹åˆ°ç»“æŸç‚¹
                    has_speech = True
                    has_end = True
        
        return has_speech, has_start, has_end
    
    def _detect_speech_vad(self, audio: np.ndarray) -> Tuple[bool, bool, bool, Optional[list]]:
        """
        ä½¿ç”¨VADæ£€æµ‹è¯­éŸ³
        
        Returns:
            Tuple[bool, bool, bool, Optional[list]]: 
                (æœ‰è¯­éŸ³, æ£€æµ‹åˆ°å¼€å§‹, æ£€æµ‹åˆ°ç»“æŸ, VADæ—¶é—´æ®µåˆ—è¡¨)
        """
        try:
            result = self.vad_model.generate(
                input=audio,
                cache={},
                is_final=False,
                chunk_size=len(audio),
            )
            
            if not result or len(result) == 0:
                return False, False, False, []
            
            res = result[0]
            
            # è§£æVADç»“æœ
            if 'value' in res:
                vad_segments = res['value']
                has_speech, has_start, has_end = self._parse_vad_output(vad_segments)
                return has_speech, has_start, has_end, vad_segments
            
            return False, False, False, []
            
        except Exception as e:
            if DEBUG:
                print(f"âš ï¸ VADé”™è¯¯: {e}")
            return False, False, False, []
    
    def _format_vad_segments(self, segments: List) -> str:
        """æ ¼å¼åŒ–VADæ®µç”¨äºæ˜¾ç¤º"""
        if not segments:
            return "[]"
        
        parts = []
        for seg in segments:
            if len(seg) >= 2:
                beg, end = seg[0], seg[1]
                if beg == -1:
                    parts.append(f"[-, {end}]")
                elif end == -1:
                    parts.append(f"[{beg}, -]")
                else:
                    parts.append(f"[{beg}, {end}]")
        
        return ", ".join(parts)
    
    def _recognize(self, audio: np.ndarray) -> str:
        """è¯†åˆ«è¯­éŸ³"""
        try:
            result = self.asr_model.generate(
                input=audio,
                batch_size=1
            )
            
            if result and len(result) > 0 and 'text' in result[0]:
                return result[0]['text'].strip()
            
            return ""
            
        except Exception as e:
            if DEBUG:
                print(f"âŒ ASRé”™è¯¯: {e}")
            return ""
    
    def _save_audio(self, audio: np.ndarray, prefix: str = "voice"):
        """ä¿å­˜éŸ³é¢‘åˆ°æ–‡ä»¶"""
        if not SAVE_AUDIO:
            return
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{prefix}_{timestamp}.wav"
            filepath = os.path.join(AUDIO_SAVE_PATH, filename)
            sf.write(filepath, audio, self.sr)
            
            if DEBUG:
                print(f"ğŸ’¾ éŸ³é¢‘å·²ä¿å­˜: {filepath}")
        
        except Exception as e:
            if DEBUG:
                print(f"âš ï¸ ä¿å­˜éŸ³é¢‘å¤±è´¥: {e}")
    
    async def get_voice_input(
        self,
        timeout: float = DEFAULT_TIMEOUT,
        prompt: Optional[str] = None,
        use_volume_threshold: bool = True
    ) -> str:
        """
        è·å–è¯­éŸ³è¾“å…¥
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´
            prompt: æç¤ºä¿¡æ¯
            use_volume_threshold: æ˜¯å¦ä½¿ç”¨éŸ³é‡é˜ˆå€¼è¿‡æ»¤
            
        Returns:
            str: è¯†åˆ«çš„æ–‡æœ¬
        """
        if prompt:
            print(prompt)
        elif DEBUG:
            print("ğŸ¤ è¯·è¯´è¯...")
        
        # é‡ç½®VADçŠ¶æ€
        self.vad_speech_started = False
        self.vad_speech_ended = False
        
        # å½•éŸ³çŠ¶æ€
        speech_buffer = []
        silence_chunks = 0
        total_chunks = 0
        accumulated_time = 0
        
        # è®¡ç®—å‚æ•°
        chunk_duration = self.vad_chunk_size / self.sr
        chunk_duration_ms = chunk_duration * 1000
        max_silence_chunks = int(SILENCE_DURATION / chunk_duration)
        min_speech_chunks = int(MIN_SPEECH_DURATION / chunk_duration)
        max_total_chunks = int(MAX_RECORDING_DURATION / chunk_duration)
        
        # å¯åŠ¨éŸ³é¢‘æµ
        stream = sd.InputStream(
            samplerate=self.sr,
            channels=CHANNELS,
            dtype="float32",
            callback=self._audio_callback,
            blocksize=self.vad_chunk_size
        )
        
        try:
            stream.start()
            self.is_active = True
            start_time = asyncio.get_event_loop().time()
            
            while True:
                # è¶…æ—¶æ£€æŸ¥
                if asyncio.get_event_loop().time() - start_time > timeout:
                    if DEBUG:
                        print("â±ï¸ è¶…æ—¶")
                    return ""
                
                # å½•éŸ³æ—¶é•¿æ£€æŸ¥
                if total_chunks >= max_total_chunks:
                    if DEBUG:
                        print("â±ï¸ è¾¾åˆ°æœ€å¤§å½•éŸ³æ—¶é•¿")
                    break
                
                # è·å–éŸ³é¢‘
                try:
                    audio_chunk = self.audio_queue.get(timeout=0.1)
                except queue.Empty:
                    await asyncio.sleep(0.05)
                    continue
                
                total_chunks += 1
                accumulated_time += chunk_duration_ms
                
                # éŸ³é‡æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
                if use_volume_threshold:
                    volume, volume_db = self._check_volume(audio_chunk)
                    if volume_db < VOLUME_THRESHOLD_DB:
                        # éŸ³é‡å¤ªä½ï¼Œè·³è¿‡VADæ£€æµ‹
                        if self.vad_speech_started and not self.vad_speech_ended:
                            silence_chunks += 1
                            speech_buffer.append(audio_chunk)
                        continue
                
                # VADæ£€æµ‹
                has_speech, detected_start, detected_end, vad_segments = \
                    self._detect_speech_vad(audio_chunk)
                
                # è°ƒè¯•ä¿¡æ¯
                if DEBUG and vad_segments:
                    segments_str = self._format_vad_segments(vad_segments)
                    status = []
                    if detected_start:
                        status.append("START")
                    if detected_end:
                        status.append("END")
                    if status:
                        print(f"  VAD: {segments_str} [{', '.join(status)}] @ {accumulated_time:.0f}ms")
                
                # æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹
                if detected_start and not self.vad_speech_started:
                    self.vad_speech_started = True
                    if DEBUG:
                        print(f"ğŸ”´ è¯­éŸ³å¼€å§‹ (æ—¶é—´: {accumulated_time:.0f}ms)")
                
                # æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸ
                if detected_end:
                    self.vad_speech_ended = True
                    if DEBUG:
                        print(f"ğŸŸ¡ VADæ£€æµ‹åˆ°ç»“æŸç‚¹ (æ—¶é—´: {accumulated_time:.0f}ms)")
                
                # çŠ¶æ€å¤„ç†
                if self.vad_speech_started:
                    speech_buffer.append(audio_chunk)
                    
                    if has_speech:
                        # æœ‰è¯­éŸ³ï¼Œé‡ç½®é™éŸ³è®¡æ•°
                        silence_chunks = 0
                    else:
                        # é™éŸ³
                        silence_chunks += 1
                        
                        if DEBUG and silence_chunks % 3 == 0:
                            silence_duration = silence_chunks * chunk_duration
                            print(f"  é™éŸ³: {silence_duration:.1f}s / {SILENCE_DURATION}s")
                    
                    # åˆ¤æ–­æ˜¯å¦ç»“æŸ
                    if silence_chunks >= max_silence_chunks or self.vad_speech_ended:
                        if len(speech_buffer) >= min_speech_chunks:
                            speech_duration = len(speech_buffer) * chunk_duration
                            if DEBUG:
                                reason = "VADç»“æŸ" if self.vad_speech_ended else "é™éŸ³è¶…æ—¶"
                                print(f"â¹ï¸ è¯­éŸ³ç»“æŸ ({reason}, æ—¶é•¿: {speech_duration:.1f}s)")
                            break
                        else:
                            if DEBUG:
                                print("âš ï¸ è¯­éŸ³å¤ªçŸ­ï¼Œç»§ç»­...")
                            self.vad_speech_started = False
                            self.vad_speech_ended = False
                            speech_buffer = []
                            silence_chunks = 0
                
                await asyncio.sleep(0.01)
            
            stream.stop()
            self.is_active = False
            
            # å¤„ç†å½•éŸ³
            if not speech_buffer:
                return ""
            
            full_audio = np.concatenate(speech_buffer)
            
            # ä¿å­˜éŸ³é¢‘
            self._save_audio(full_audio)
            
            # è¯†åˆ«
            if DEBUG:
                print("ğŸ”„ æ­£åœ¨è¯†åˆ«...")
            
            text = self._recognize(full_audio)
            
            if text and DEBUG:
                print(f"âœ… è¯†åˆ«: {text}")
            elif DEBUG:
                print("âŒ æœªè¯†åˆ«åˆ°å†…å®¹")
            
            return text
            
        except Exception as e:
            if DEBUG:
                print(f"âŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return ""
            
        finally:
            if stream.active:
                stream.stop()
            stream.close()
            self.is_active = False
            
            # æ¸…ç©ºé˜Ÿåˆ—
            while not self.audio_queue.empty():
                try:
                    self.audio_queue.get_nowait()
                except queue.Empty:
                    break


# å…¨å±€å®ä¾‹
_handler: Optional[EnhancedVoiceInputHandler] = None


def get_handler() -> EnhancedVoiceInputHandler:
    """è·å–å¤„ç†å™¨å•ä¾‹"""
    global _handler
    if _handler is None:
        _handler = EnhancedVoiceInputHandler()
    return _handler


async def get_voice_input(
    timeout: float = DEFAULT_TIMEOUT,
    prompt: Optional[str] = None
) -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šè·å–è¯­éŸ³è¾“å…¥
    
    Example:
        >>> text = await get_voice_input("è¯·è¯´å‡ºæŒ‡ä»¤:")
        >>> print(f"è¯†åˆ«åˆ°: {text}")
    """
    handler = get_handler()
    return await handler.get_voice_input(timeout=timeout, prompt=prompt)


if __name__ == "__main__":
    async def test():
        print("=== æµ‹è¯•å¢å¼ºç‰ˆè¯­éŸ³è¾“å…¥ ===\n")
        print("VADè¾“å‡ºæ ¼å¼è¯´æ˜:")
        print("  [[beg, end]] - å®Œæ•´è¯­éŸ³æ®µ")
        print("  [[beg, -]]   - æ£€æµ‹åˆ°å¼€å§‹")
        print("  [[-, end]]   - æ£€æµ‹åˆ°ç»“æŸ")
        print("  []           - æ— è¯­éŸ³\n")
        
        result = await get_voice_input("è¯·è¯´è¯:")
        print(f"\næœ€ç»ˆç»“æœ: '{result}'")
    
    asyncio.run(test())