import sounddevice as sd
import numpy as np
from funasr import AutoModel
import asyncio
from typing import Optional, Tuple
import queue
import os
from datetime import datetime
import soundfile as sf

from voice_config import *


class ImprovedVoiceInputHandler:
    """æ”¹è¿›çš„è¯­éŸ³è¾“å…¥å¤„ç†å™¨"""
    
    def __init__(self):
        self.sr = SAMPLE_RATE
        self.vad_chunk_size = VAD_CHUNK_SIZE
        
        # åŠ è½½æ¨¡å‹
        if DEBUG:
            print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        self.vad_model = AutoModel(model=VAD_MODEL, disable_pbar=True)
        self.asr_model = AutoModel(model=ASR_MODEL)
        
        if DEBUG:
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # çŠ¶æ€
        self.audio_queue = queue.Queue()
        self.is_active = False
        
        # å¦‚æœéœ€è¦ä¿å­˜éŸ³é¢‘ï¼Œåˆ›å»ºç›®å½•
        if SAVE_AUDIO:
            os.makedirs(AUDIO_SAVE_PATH, exist_ok=True)
    
    def _audio_callback(self, indata, frames, time, status):
        """éŸ³é¢‘æµå›è°ƒ"""
        if status and DEBUG:
            print(f"âš ï¸ éŸ³é¢‘çŠ¶æ€: {status}")
        
        audio_chunk = indata[:, 0].copy()
        self.audio_queue.put(audio_chunk)
    
    def _check_volume(self, audio: np.ndarray) -> Tuple[float, float]:
        """
        æ£€æŸ¥éŸ³é‡
        
        Returns:
            Tuple[float, float]: (éŸ³é‡å‡å€¼, éŸ³é‡dB)
        """
        volume = np.abs(audio).mean()
        volume_db = 20 * np.log10(volume + 1e-10)
        return volume, volume_db
    
    def _detect_speech_vad(self, audio: np.ndarray) -> Tuple[bool, Optional[list]]:
        """
        ä½¿ç”¨VADæ£€æµ‹è¯­éŸ³
        
        Returns:
            Tuple[bool, Optional[list]]: (æ˜¯å¦æ£€æµ‹åˆ°è¯­éŸ³, VADæ—¶é—´æ®µåˆ—è¡¨)
            VADæ—¶é—´æ®µæ ¼å¼: 
            - [[beg1, end1], [beg2, end2], ...]: å®Œæ•´çš„è¯­éŸ³æ®µ
            - [[beg, -1]]: åªæ£€æµ‹åˆ°èµ·å§‹ç‚¹
            - [[-1, end]]: åªæ£€æµ‹åˆ°ç»“æŸç‚¹
            - []: æ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³
        """
        try:
            result = self.vad_model.generate(
                input=audio,
                cache={},
                is_final=False,
                chunk_size=len(audio),
            )
            
            if not result or len(result) == 0:
                return False, []
            
            res = result[0]
            
            # è§£æVADç»“æœ
            if 'value' in res:
                vad_segments = res['value']
                
                if not vad_segments or len(vad_segments) == 0:
                    # ç©ºåˆ—è¡¨ï¼šæ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³
                    return False, []
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„è¯­éŸ³æ®µ
                has_speech = False
                for segment in vad_segments:
                    if len(segment) >= 2:
                        beg, end = segment[0], segment[1]
                        
                        # [[beg, -1]]: æ£€æµ‹åˆ°èµ·å§‹ç‚¹
                        if beg >= 0 and end == -1:
                            has_speech = True
                            break
                        
                        # [[beg, end]]: å®Œæ•´çš„è¯­éŸ³æ®µ
                        if beg >= 0 and end > beg:
                            has_speech = True
                            break
                        
                        # [[-1, end]]: æ£€æµ‹åˆ°ç»“æŸç‚¹ï¼ˆè¯´æ˜ä¹‹å‰æœ‰è¯­éŸ³ï¼‰
                        if beg == -1 and end >= 0:
                            has_speech = True
                            break
                
                return has_speech, vad_segments
            
            return False, []
            
        except Exception as e:
            if DEBUG:
                print(f"âš ï¸ VADé”™è¯¯: {e}")
            return False, []
    
    def _detect_speech_combined(self, audio: np.ndarray) -> Tuple[bool, Optional[list]]:
        """
        ç»¼åˆæ£€æµ‹ï¼šVAD + éŸ³é‡
        
        Returns:
            Tuple[bool, Optional[list]]: (æ˜¯å¦æ£€æµ‹åˆ°è¯­éŸ³, VADæ—¶é—´æ®µåˆ—è¡¨)
        """
        # éŸ³é‡æ£€æµ‹
        volume, volume_db = self._check_volume(audio)
        
        if volume_db < VOLUME_THRESHOLD_DB:
            return False, []  # éŸ³é‡å¤ªå°
        
        # VADæ£€æµ‹
        return self._detect_speech_vad(audio)
    
    def _recognize(self, audio: np.ndarray) -> str:
        """è¯†åˆ«è¯­éŸ³"""
        try:
            result = self.asr_model.generate(
                input=audio,
                batch_size=1
            )
            
            if result and len(result) > 0 and 'text' in result[0]:
                return result[0]['text'].strip()
            
            return ""
            
        except Exception as e:
            if DEBUG:
                print(f"âŒ ASRé”™è¯¯: {e}")
            return ""
    
    def _save_audio(self, audio: np.ndarray, prefix: str = "voice"):
        """ä¿å­˜éŸ³é¢‘åˆ°æ–‡ä»¶"""
        if not SAVE_AUDIO:
            return
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{prefix}_{timestamp}.wav"
            filepath = os.path.join(AUDIO_SAVE_PATH, filename)
            sf.write(filepath, audio, self.sr)
            
            if DEBUG:
                print(f"ğŸ’¾ éŸ³é¢‘å·²ä¿å­˜: {filepath}")
        
        except Exception as e:
            if DEBUG:
                print(f"âš ï¸ ä¿å­˜éŸ³é¢‘å¤±è´¥: {e}")
    
    async def get_voice_input(
        self,
        timeout: float = DEFAULT_TIMEOUT,
        prompt: Optional[str] = None,
        use_volume_detection: bool = True
    ) -> str:
        """
        è·å–è¯­éŸ³è¾“å…¥
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´
            prompt: æç¤ºä¿¡æ¯
            use_volume_detection: æ˜¯å¦ä½¿ç”¨éŸ³é‡æ£€æµ‹è¾…åŠ©VAD
            
        Returns:
            str: è¯†åˆ«çš„æ–‡æœ¬
        """
        if prompt:
            print(prompt)
        elif DEBUG:
            print("ğŸ¤ è¯·è¯´è¯...")
        
        # çŠ¶æ€é‡ç½®
        speech_detected = False
        silence_chunks = 0
        speech_buffer = []
        total_chunks = 0
        accumulated_time = 0  # ç´¯è®¡æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        
        # è®¡ç®—å‚æ•°
        chunk_duration = self.vad_chunk_size / self.sr
        chunk_duration_ms = chunk_duration * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        max_silence_chunks = int(SILENCE_DURATION / chunk_duration)
        min_speech_chunks = int(MIN_SPEECH_DURATION / chunk_duration)
        max_total_chunks = int(MAX_RECORDING_DURATION / chunk_duration)
        
        # å¯åŠ¨éŸ³é¢‘æµ
        stream = sd.InputStream(
            samplerate=self.sr,
            channels=CHANNELS,
            dtype="float32",
            callback=self._audio_callback,
            blocksize=self.vad_chunk_size
        )
        
        try:
            stream.start()
            self.is_active = True
            start_time = asyncio.get_event_loop().time()
            
            while True:
                # è¶…æ—¶æ£€æŸ¥
                if asyncio.get_event_loop().time() - start_time > timeout:
                    if DEBUG:
                        print("â±ï¸ è¶…æ—¶")
                    return ""
                
                # å½•éŸ³æ—¶é•¿æ£€æŸ¥
                if total_chunks >= max_total_chunks:
                    if DEBUG:
                        print("â±ï¸ è¾¾åˆ°æœ€å¤§å½•éŸ³æ—¶é•¿")
                    break
                
                # è·å–éŸ³é¢‘
                try:
                    audio_chunk = self.audio_queue.get(timeout=0.1)
                except queue.Empty:
                    await asyncio.sleep(0.05)
                    continue
                
                total_chunks += 1
                accumulated_time += chunk_duration_ms
                
                # æ£€æµ‹è¯­éŸ³
                if use_volume_detection:
                    has_speech, vad_segments = self._detect_speech_combined(audio_chunk)
                else:
                    has_speech, vad_segments = self._detect_speech_vad(audio_chunk)
                
                # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºVADæ£€æµ‹ç»“æœ
                if DEBUG and vad_segments:
                    if has_speech:
                        # æ ¼å¼åŒ–æ˜¾ç¤ºVADæ®µ
                        segments_str = ", ".join([f"[{s[0]}, {s[1]}]" for s in vad_segments])
                        print(f"  VADæ£€æµ‹: {segments_str} (ç´¯è®¡æ—¶é—´: {accumulated_time:.0f}ms)")
                
                if has_speech:
                    if not speech_detected:
                        if DEBUG:
                            print(f"ğŸ”´ è¯­éŸ³å¼€å§‹ (æ—¶é—´: {accumulated_time:.0f}ms)")
                        speech_detected = True
                    
                    silence_chunks = 0
                    speech_buffer.append(audio_chunk)
                    
                elif speech_detected:
                    silence_chunks += 1
                    speech_buffer.append(audio_chunk)
                    
                    # æ˜¾ç¤ºé™éŸ³è®¡æ•°
                    if DEBUG and silence_chunks % 3 == 0:  # æ¯3ä¸ªchunkæ˜¾ç¤ºä¸€æ¬¡
                        silence_duration = silence_chunks * chunk_duration
                        print(f"  é™éŸ³: {silence_duration:.1f}s / {SILENCE_DURATION}s")
                    
                    if silence_chunks >= max_silence_chunks:
                        if len(speech_buffer) >= min_speech_chunks:
                            speech_duration = len(speech_buffer) * chunk_duration
                            if DEBUG:
                                print(f"â¹ï¸ è¯­éŸ³ç»“æŸ (æ—¶é•¿: {speech_duration:.1f}s)")
                            break
                        else:
                            if DEBUG:
                                print("âš ï¸ è¯­éŸ³å¤ªçŸ­ï¼Œç»§ç»­...")
                            speech_detected = False
                            speech_buffer = []
                            silence_chunks = 0
                
                await asyncio.sleep(0.01)
            
            stream.stop()
            self.is_active = False
            
            # å¤„ç†å½•éŸ³
            if not speech_buffer:
                return ""
            
            full_audio = np.concatenate(speech_buffer)
            
            # ä¿å­˜éŸ³é¢‘(å¦‚æœå¯ç”¨)
            self._save_audio(full_audio)
            
            # è¯†åˆ«
            if DEBUG:
                print("ğŸ”„ æ­£åœ¨è¯†åˆ«...")
            
            text = self._recognize(full_audio)
            
            if text and DEBUG:
                print(f"âœ… è¯†åˆ«: {text}")
            elif DEBUG:
                print("âŒ æœªè¯†åˆ«åˆ°å†…å®¹")
            
            return text
            
        except Exception as e:
            if DEBUG:
                print(f"âŒ é”™è¯¯: {e}")
            return ""
            
        finally:
            if stream.active:
                stream.stop()
            stream.close()
            self.is_active = False
            
            # æ¸…ç©ºé˜Ÿåˆ—
            while not self.audio_queue.empty():
                try:
                    self.audio_queue.get_nowait()
                except queue.Empty:
                    break


# å…¨å±€å®ä¾‹
_handler: Optional[ImprovedVoiceInputHandler] = None


def get_handler() -> ImprovedVoiceInputHandler:
    """è·å–å¤„ç†å™¨å•ä¾‹"""
    global _handler
    if _handler is None:
        _handler = ImprovedVoiceInputHandler()
    return _handler


async def get_voice_input(
    timeout: float = DEFAULT_TIMEOUT,
    prompt: Optional[str] = None
) -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šè·å–è¯­éŸ³è¾“å…¥
    
    Example:
        >>> text = await get_voice_input("è¯·è¯´å‡ºæŒ‡ä»¤:")
        >>> print(f"è¯†åˆ«åˆ°: {text}")
    """
    handler = get_handler()
    return await handler.get_voice_input(timeout=timeout, prompt=prompt)


if __name__ == "__main__":
    async def test():
        print("=== æµ‹è¯•è¯­éŸ³è¾“å…¥ ===\n")
        
        result = await get_voice_input("è¯·è¯´è¯:")
        print(f"\nç»“æœ: '{result}'")
    
    asyncio.run(test())