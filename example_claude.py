import asyncio
import json
import re
import os

from typing import TypedDict, Annotated, Optional, Literal, List
from dotenv import load_dotenv

from langchain.agents import AgentExecutor, create_structured_chat_agent
from langchain.chat_models import init_chat_model
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableWithMessageHistory
from langchain_core.tools import BaseTool
from langchain_mcp_adapters.client import MultiServerMCPClient

from langgraph.graph import add_messages, StateGraph, END
from langgraph.prebuilt import create_react_agent

from db.chats import get_session_history
from template import system_template, router_template

# åˆå§‹åŒ–æ¨¡å‹
load_dotenv()
api_key = os.getenv("BAILIAN_API_KEY")

model = init_chat_model(
    "qwen3-0.6b",
    model_provider="openai",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=api_key,
    temperature=1,
    model_kwargs={"extra_body": {"enable_thinking": False}}
)

prompt = ChatPromptTemplate.from_messages([
    ("system", system_template),
    MessagesPlaceholder("chat_history"),
    ("human", "{input} {agent_scratchpad}"),
])

# æ”¹è¿›çš„è·¯ç”±æ¨¡æ¿



class AgentState(TypedDict):
    """çŠ¶æ€å®šä¹‰"""
    messages: Annotated[list, add_messages]
    sub_tasks: List[dict]  # [{"task": "xxx", "category": "xxx"}]
    current_idx: int  # å½“å‰æ‰§è¡Œåˆ°ç¬¬å‡ ä¸ªä»»åŠ¡
    input: str  # åŸå§‹ç”¨æˆ·è¾“å…¥
    is_chat: bool  # æ˜¯å¦ä¸ºæ™®é€šå¯¹è¯


_tools_cache = None


async def load_mcp_tools():
    """åŠ è½½ MCP å·¥å…·"""
    global _tools_cache
    if _tools_cache is None:
        client = MultiServerMCPClient(
            {
                "wisehome": {
                    "url": "http://localhost:8001/sse",
                    "transport": "sse",
                }
            }
        )
        _tools_cache = await client.get_tools()
    return _tools_cache


def extract_json(text: str) -> dict:
    """ä»æ–‡æœ¬ä¸­æå– JSON"""
    text = text.strip()

    # å°è¯•ç›´æ¥è§£æ
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    # å°è¯•æå– JSON å—
    match = re.search(r'\{.*\}', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(0))
        except Exception as e:
            print(f"âš ï¸  JSON è§£æå¤±è´¥ï¼š{e}")

    return {}


async def llm_route(user_input: str) -> dict:
    """ä½¿ç”¨ LLM åˆ¤æ–­æ„å›¾å¹¶æ‹†åˆ†ä»»åŠ¡"""
    route_prompt = ChatPromptTemplate.from_messages([
        ("system", router_template),
        ("user", "{input}")
    ])

    response = await (route_prompt | model).ainvoke({"input": user_input})
    result = extract_json(response.content)

    # éªŒè¯ç»“æœ
    if not result:
        print("âš ï¸  è·¯ç”±è§£æå¤±è´¥ï¼Œé»˜è®¤ä¸ºæ™®é€šå¯¹è¯")
        return {"type": "chat", "response": "æŠ±æ­‰ï¼Œæˆ‘æ²¡ç†è§£ä½ çš„æ„æ€ã€‚"}

    result_type = result.get("type", "chat")

    # å¦‚æœæ˜¯ä»»åŠ¡ä½† sub_tasks ä¸ºç©ºï¼Œé™çº§ä¸ºå¯¹è¯
    if result_type == "task" and not result.get("sub_tasks"):
        print("âš ï¸  ä»»åŠ¡æ‹†åˆ†ä¸ºç©ºï¼Œé™çº§ä¸ºå¯¹è¯")
        return {"type": "chat", "response": "è¯·å‘Šè¯‰æˆ‘ä½ éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Ÿ"}

    return result


async def agent_router(state: AgentState) -> AgentState:
    """è·¯ç”±èŠ‚ç‚¹ï¼šåˆ¤æ–­æ„å›¾å¹¶åˆå§‹åŒ–çŠ¶æ€"""
    user_input = state["input"]
    route_result = await llm_route(user_input)

    result_type = route_result.get("type", "chat")

    print(f"ğŸ“‹ æ„å›¾è¯†åˆ«ï¼š{result_type}")

    if result_type == "chat":
        # æ™®é€šå¯¹è¯ï¼Œç›´æ¥è¿”å›
        response = route_result.get("response", "ä½ å¥½ï¼")
        print(f"ğŸ’¬ å¯¹è¯å›å¤ï¼š{response}")
        return {
            **state,
            "is_chat": True,
            "messages": [AIMessage(content=response)],
            "sub_tasks": [],
            "current_idx": 0
        }
    else:
        # ä»»åŠ¡æŒ‡ä»¤
        sub_tasks = route_result.get("sub_tasks", [])
        print(f"ğŸ“‹ ä»»åŠ¡æ‹†åˆ†ï¼š{json.dumps(sub_tasks, ensure_ascii=False)}")
        return {
            **state,
            "is_chat": False,
            "sub_tasks": sub_tasks,
            "current_idx": 0,
            "messages": []
        }


def route_decision(state: AgentState) -> Literal["smart_home_control", "query_info", "end"]:
    """å†³ç­–å‡½æ•°ï¼šæ ¹æ®å½“å‰ä»»åŠ¡çš„ category è·¯ç”±"""
    # å¦‚æœæ˜¯æ™®é€šå¯¹è¯ï¼Œç›´æ¥ç»“æŸ
    if state.get("is_chat", False):
        return "end"

    current_idx = state.get("current_idx", 0)
    sub_tasks = state.get("sub_tasks", [])

    # æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•
    if current_idx >= len(sub_tasks):
        return "end"

    # è·å–å½“å‰ä»»åŠ¡çš„ç±»åˆ«
    current_task = sub_tasks[current_idx]
    category = current_task.get("category", "query_info")

    print(f"ğŸ”€ è·¯ç”±åˆ°: {category} (ä»»åŠ¡ {current_idx + 1}/{len(sub_tasks)})")

    return category


async def filter_tools_by_category(all_tools: List[BaseTool], category: str) -> List[BaseTool]:
    """æ ¹æ®ç±»åˆ«ç­›é€‰å·¥å…·"""
    if category == "smart_home_control":
        keywords = ["turn", "set", "open", "close", "switch", "è°ƒèŠ‚", "å¼€", "å…³", "è®¾ç½®", "play", "stop"]
    else:  # query_info
        keywords = ["get", "query", "status", "weather", "list", "æŸ¥è¯¢", "è·å–", "çŠ¶æ€", "ä¿¡æ¯"]

    filtered_tools = [
        tool for tool in all_tools
        if any(kw in tool.name.lower() or kw in tool.description.lower() for kw in keywords)
    ]

    # å…œåº•ï¼šè‹¥æ²¡ç­›åˆ°å·¥å…·ï¼Œè¿”å›æ‰€æœ‰å·¥å…·
    return filtered_tools if filtered_tools else all_tools


async def execute_current_task(state: AgentState, category: str) -> AgentState:
    """æ‰§è¡Œå½“å‰å­ä»»åŠ¡"""
    current_idx = state["current_idx"]
    sub_tasks = state["sub_tasks"]
    current_task = sub_tasks[current_idx]
    task_content = current_task["task"]

    print(f"âš™ï¸  æ‰§è¡Œä»»åŠ¡ {current_idx + 1}: {task_content} [{category}]")

    # åŠ è½½å¹¶ç­›é€‰å·¥å…·
    all_tools = await load_mcp_tools()
    filtered_tools = await filter_tools_by_category(all_tools, category)

    # æ˜¾ç¤ºå·¥å…·åç§°ï¼ˆè°ƒè¯•ç”¨ï¼‰
    tool_names = [t.name for t in filtered_tools]
    print(f"ğŸ”§ ä½¿ç”¨å·¥å…·: {', '.join(tool_names)}")

    # åˆ›å»º Agent
    agent = create_structured_chat_agent(model, tools=filtered_tools, prompt=prompt)
    agent_executor = AgentExecutor.from_agent_and_tools(
        agent=agent,
        tools=filtered_tools,
        verbose=True,
        handle_parsing_errors=True,
    )

    agent_with_memory = RunnableWithMessageHistory(
        agent_executor,
        get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
    )

    try:
        response = await agent_with_memory.ainvoke(
            {"input": task_content},
            config={"configurable": {"thread_id": "2", "session_id": "user_1"}}
        )
        result = response.get("output", "æ‰§è¡Œå®Œæˆ")

        print(f"âœ… ä»»åŠ¡å®Œæˆ: {result}")

        return {
            **state,
            "messages": [*state["messages"], AIMessage(content=f"[ä»»åŠ¡{current_idx + 1}] {result}")],
            "current_idx": current_idx + 1
        }
    except Exception as e:
        print(f"âŒ ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
        return {
            **state,
            "messages": [*state["messages"], AIMessage(content=f"[ä»»åŠ¡{current_idx + 1}] å¤±è´¥: {str(e)}")],
            "current_idx": current_idx + 1
        }


async def smart_home_agent(state: AgentState) -> AgentState:
    """æ™ºèƒ½å®¶å±…æ§åˆ¶æ‰§è¡Œå™¨"""
    return await execute_current_task(state, "smart_home_control")


async def query_info_agent(state: AgentState) -> AgentState:
    """ä¿¡æ¯æŸ¥è¯¢æ‰§è¡Œå™¨"""
    return await execute_current_task(state, "query_info")


async def create_workflow():
    """æ„å»º Workflow"""
    workflow = StateGraph(AgentState)

    workflow.add_node("router", agent_router)
    workflow.add_node("smart_home_control", smart_home_agent)
    workflow.add_node("query_info", query_info_agent)

    workflow.set_entry_point("router")

    # ä» router åˆ°ç¬¬ä¸€ä¸ªä»»åŠ¡æˆ–ç»“æŸ
    workflow.add_conditional_edges(
        "router",
        route_decision,
        {
            "smart_home_control": "smart_home_control",
            "query_info": "query_info",
            "end": END
        }
    )

    # æ¯ä¸ªä»»åŠ¡æ‰§è¡Œå®Œåï¼Œç»§ç»­è·¯ç”±åˆ°ä¸‹ä¸€ä¸ªä»»åŠ¡æˆ–ç»“æŸ
    workflow.add_conditional_edges(
        "smart_home_control",
        route_decision,
        {
            "smart_home_control": "smart_home_control",
            "query_info": "query_info",
            "end": END
        }
    )

    workflow.add_conditional_edges(
        "query_info",
        route_decision,
        {
            "smart_home_control": "smart_home_control",
            "query_info": "query_info",
            "end": END
        }
    )

    return workflow.compile()


async def main():
    try:
        print("ğŸ  MCP æ™ºèƒ½å®¶å±…ç³»ç»Ÿå·²è¿æ¥ï¼")
        print("ğŸ’¡ ç¤ºä¾‹ï¼š'æ‰“å¼€å®¢å…ç¯å¹¶æŸ¥è¯¢å¤©æ°”' æˆ– 'ä½ å¥½'")

        workflow = await create_workflow()

        while True:
            user_input = input("\nä½ : ").strip()

            if user_input.lower() in {"exit", "quit", "é€€å‡º"}:
                print("ğŸ‘‹ å†è§ï¼")
                break

            if not user_input:
                continue

            response = await workflow.ainvoke(
                {"input": user_input},
                config={"configurable": {"thread_id": "2", "session_id": "user_1"}}
            )

            messages = response.get("messages", [])
            if messages:
                print("\nğŸ¤– AI:")
                for msg in messages:
                    if hasattr(msg, "content"):
                        print(f"  {msg.content}")
            else:
                print("ğŸ¤– AI: (æ— å›å¤)")

    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯ï¼š{e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ å·²é€€å‡ºã€‚")